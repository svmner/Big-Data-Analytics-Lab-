{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import sys \n",
    "from pyspark import SparkContext\n",
    "from pyspark import SparkConf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Filter and withColumn on a Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "| id|  name|\n",
      "+---+------+\n",
      "|  1| Alice|\n",
      "|  2|   Bob|\n",
      "|  3| Carol|\n",
      "|  4|Aarons|\n",
      "|  5|   Ave|\n",
      "+---+------+\n",
      "\n",
      "+---+------+\n",
      "| id|  name|\n",
      "+---+------+\n",
      "|  1| Alice|\n",
      "|  4|Aarons|\n",
      "|  5|   Ave|\n",
      "+---+------+\n",
      "\n",
      "+---+------+---+\n",
      "| id|  name|age|\n",
      "+---+------+---+\n",
      "|  1| Alice|  7|\n",
      "|  2|   Bob| 17|\n",
      "|  3| Carol| 27|\n",
      "|  4|Aarons| 37|\n",
      "|  5|   Ave| 47|\n",
      "+---+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "df = spark.createDataFrame([(1,\"Alice\"),\n",
    "                            (2, \"Bob\"),\n",
    "                            (3, \"Carol\"),\n",
    "                            (4, \"Aarons\"),\n",
    "                            (5, \"Ave\")],\n",
    "                            [\"id\",\"name\"])\n",
    "\n",
    "df.show()\n",
    "\n",
    "df1 = df.filter(df[\"name\"].startswith('A'))\n",
    "df1.show()\n",
    "\n",
    "df2 = df.withColumn(\"age\", (df[\"id\"] * 10) - random.randint(2,6))\n",
    "df2.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count on Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+---+\n",
      "| id|  name|age|\n",
      "+---+------+---+\n",
      "|  1| Alice|  7|\n",
      "|  2|   Bob| 17|\n",
      "|  3| Carol| 27|\n",
      "|  4|Aarons| 37|\n",
      "|  5|   Ave| 47|\n",
      "+---+------+---+\n",
      "\n",
      "Count = 5\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "df = df2\n",
    "\n",
    "df.show()\n",
    "\n",
    "print(f'Count = {df.count()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sum,Average on Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+---+\n",
      "| id|  name|age|\n",
      "+---+------+---+\n",
      "|  1| Alice|  7|\n",
      "|  2|   Bob| 17|\n",
      "|  3| Carol| 27|\n",
      "|  4|Aarons| 37|\n",
      "|  5|   Ave| 47|\n",
      "+---+------+---+\n",
      "\n",
      "Sum of Ages: 135\n",
      "Average of Ages: 27.0\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "df2.show()\n",
    "\n",
    "sum = df2.groupBy().sum().collect()[0][1]\n",
    "\n",
    "print(f'Sum of Ages: {sum}')\n",
    "\n",
    "avg = (df2.groupBy().sum().collect()[0][1])/(df2.count())\n",
    "\n",
    "print(f'Average of Ages: {avg}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write PySpark Dataframe to CSV File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+---+\n",
      "| id|  name|age|\n",
      "+---+------+---+\n",
      "|  1| Alice|  7|\n",
      "|  2|   Bob| 17|\n",
      "|  3| Carol| 27|\n",
      "|  4|Aarons| 37|\n",
      "|  5|   Ave| 47|\n",
      "+---+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "df_final = df2\n",
    "\n",
    "df_final.show()\n",
    "\n",
    "output_path = \"/home/lplab/Desktop/210962021/Lab2/output.csv\"\n",
    "\n",
    "df_final.repartition(1).write.csv(output_path,header = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement Word Count Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|word      |count|\n",
      "+----------+-----+\n",
      "|Processing|1    |\n",
      "|example   |5    |\n",
      "|count     |4    |\n",
      "|data      |1    |\n",
      "|Hello     |1    |\n",
      "|word      |1    |\n",
      "|sample    |1    |\n",
      "|PySpark   |4    |\n",
      "|world     |3    |\n",
      "|text      |2    |\n",
      "|Word      |1    |\n",
      "+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as f \n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "lines = spark.read.text(\"wordcount.txt\")\n",
    "\n",
    "words = lines.withColumn('word',f.explode(f.split(f.col('value'),' ')))\n",
    "\n",
    "word_counts = words.groupBy('word').agg(f.count('word').alias('count'))\n",
    "\n",
    "word_counts.show(word_counts.count(),truncate = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
